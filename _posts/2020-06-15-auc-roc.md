---
title: "AUC ROC"
date: 2020-06-03 5:00:00
---

### Introduction

## In this tutorial, we will be learning about the ROC curve.

- Before going through this post, it is highly recommended that you should have a good understanding of what is **confusion matrix and it's related terms.** If you want to learn more about confusion matrix please go through my youtube video, [click here](https://www.youtube.com/watch?v=wmk2R8OF2ek)
- Most of the machine learning problems you solve from online courses are mostly introductory datasets which are balanced and have a guarantee of high accuracy with most classifiers. But doesn’t reflect how data looks in the “real world”.
- Most of the **"real world"** data is not balanced. so it is important to learn how to evaluate the performance of the model where your interest lies in identifying the **minority class**.
- If it is important to check or visualize the performance of the binary class classification problem, we use **AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve**.

- In few problems, predicting probabilities is better than predicting the class itself.  
- Predicting the probabilities can help you to tune the threshold values to balance between 2 errors that model make.
    - **False Positives:** You predicted today will be a rainy day when there was no rain
    - **False Negatives:** You predicted today is not a rainy day when in fact there was rain today.
- So by tuning the threshold value you can change the behaviour of your model for the problem you are solving.
- Let's consider an example of wild animals prediction on Agri fields. Where we are more concerned with having low false negatives than low false positives. Because if we have more false negatives then we are not warning the farmers about the wild animals and their field will be damaged.

### Example

Let's use convolutional neural networks to detect animals in the frame and predict whether it is a wild animal or not. In the last layer, we will be using sigmoid as an activation function to predict the probability and predict the class(wild animal or not)

<img src="http://drive.google.com/uc?export=view&id=179E4wrpnvhbCfER2wjbRb93VkEYaUha8" height="400"/>

- If you notice the circled animals where the sheep is predicted as wild animal and lion & zebra are predicted as a domestic animals (There can be any reason like bad light, fog, etc.. for this output)

 - Now if we fit the sigmoid activation on the last layer the y-axis will become the probability of predicting the type of an animal based on logit values.

<img src="http://drive.google.com/uc?export=view&id=1KD8gsOxve6SjjqFJ4rAncKGyW2PdXgt_" height="400"/>

- In the end, we need to convert these probabilities into classifications. So we can set a threshold value to classify those prbobabilities. let's consider 0.5 as a threshold value and if probability >= threshold then it is classified as a wild animal or if the probability is < threshold then it will be classified as a domestic animal

<img src="http://drive.google.com/uc?export=view&id=1Hj69nrVnHsTOrU3QCKqKC7F_YKxx2dSy" height="400"/>

- Now let's evaluate the performance of the model with a threshold value of 0.5

<img src="http://drive.google.com/uc?export=view&id=1NSKysvkTkGmnM2VFZochcNuWCFxbIaBJ" height="400"/>

- If we see it classified all the animals correctly except 3 animals (sheep, zebra and lion)
- Where it classified sheep as a wild animal and lion & Zebra as a domestic animals.

Lets's see the confusion matrix and identify the error

- Let's first understand what is confusion matrix and how it looks

<img src="http://drive.google.com/uc?export=view&id=1XkXw1hCRvGib-D1bTNMfJ-y4he6PFpNb" height="400"/>

    True Positives(TP): It is a wild animal and our model predicted it as a wild animal
    False Positives(FP): It is a Domestic animal and our model predicted it as a wild animal
    True Negatives(TN): It is a Domestic animal and our model predicted it as Domestic animal
    False Negatives(FN): It is a Wild animal and our model predicted it as Domestic animal
- This is confusion matrix and let's replace those 4 values which we got from the model

<img src="http://drive.google.com/uc?export=view&id=1_QIm7A0yAUuB8JC699Zaca4zyWaHoEF8" height="400"/>

- This is how it looks after filling the values

- We can now evaluate the performance of the model when the threshold value is 0.5 by calculating sensitivity and specificity
    - **Sensitivity:** It gives the percentage value of animals which are classified as wild are wild.
    \begin{equation} sensitivity = \frac{True positives}{True Positives + False Negativies} \end{equation}
    \begin{equation} sensitivity = \frac{3}{3 + 2} \end{equation}
    \begin{equation} sensitivity = 0.6 \end{equation}
    - It says that 60% of wild animals were correctly classified
    - **Specificity:** It gives the percentage value of animals which are classified as domestic are actually domestic.
    \begin{equation} specificity = \frac{True Negatives}{True Negatives + False Positives} \end{equation}
    \begin{equation} specificity = \frac{3}{3 + 1} \end{equation}
    \begin{equation} specificity = 0.75 \end{equation}
    - It says that 75% of domestic animals were correctly classified


- As we discussed our model will make 2 mistakes **False Positives** and **False Negatives**.
    - If predicting positives is important for you, then we should choose a model which makes less False Negatives(Choosing a model with higher sensitivity(True positive rate))
    - If predicting negatives is important for you, then we should choose a model which makes less False Positives(Choosing a model with higher specificity (True positive rate))


- We can maximize or minimize the metric (sensitivity or specificity) by choosing the right threshold value.

-Let's try to choose different threshold values and see how our model will work.
    - let the threshold value be 0.05

<img src="http://drive.google.com/uc?export=view&id=1yozYmle6uSJGAEKbPWYqg8QXMEdkUVb_" height="400"/>

    Confusion matrix

<img src="http://drive.google.com/uc?export=view&id=17oPwdg_9f3jwVYec2dGS3DGYewLt5qPl" height="400"/>

    By Decreasing the threshold value
      False positives will be increased
      False Negatives will be decreased

    By lowering the threshold value we are giving more importance in predicting the wild animals as wild. It is important right we need to alert the farmers correctly, else they get into trouble. There is no issue even False positives are increased upto some number.

- Let's Increase the threshold value to 0.8.
<img src="http://drive.google.com/uc?export=view&id=1zATnB9cE7lkdst6L4H_Se3OQ4xM-Wkn8" height="400"/>

    Confusion matrix

<img src="http://drive.google.com/uc?export=view&id=1SJG7_lRyXcd0LWFRN4YnwcV8N9fDqNqs" height="400"/>

    By Increasing the threshold value
      False positives will be decreased
      False Negatives will be increased

    By increasing the threshold value we are giving more importance in predicting the domestic animals as domestic. There is no issue even False positives are increased in this case

You got some understanding right by varying the threshold value your result will get changed.

So which threshold value works best? It is hard to find which threshold value works better as we cannot go through every threshold value and check it's respective confusion matrix.


One way to solve this is by using **ROC(Receiver Operator Characteristic) Curve**



If you like it please give a star for <a href="https://github.com/udaykiranreddykondreddy/Code-for-learn-machinelearning" target="_blank">this</a> repository.

### Code

<a href="https://github.com/udaykondreddy/Code-for-learn-machinelearning/blob/master/math/statistics/mean_mode_median.ipynb"  class="btn btn-info" role="button" target="_blank"> <i class="fa fa-github fa-2x" aria-hidden="true"></i></a>
